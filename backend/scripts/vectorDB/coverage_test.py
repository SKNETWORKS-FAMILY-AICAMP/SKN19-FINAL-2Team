import pandas as pd
import json
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics.pairwise import cosine_similarity
import scipy.stats

# 1. ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬
def run_performance_analysis(file_path='backend/scripts/vectorDB/raw/notes_mapping_final2.json'):
    accords_33 = [
        'Animal', 'Aquatic', 'Chypre', 'Citrus', 'Creamy', 'Earthy', 'Floral', 'FougÃ¨re', 
        'Fresh', 'Fruity', 'Gourmand', 'Green', 'Leathery', 'Oriental', 'Powdery', 
        'Resinous', 'Smoky', 'Spicy', 'Sweet', 'Synthetic', 'Woody',
        'Alcoholic', 'Aldehydic', 'Bitter', 'Chemical', 'Herbal', 'Metallic', 'Minty', 
        'Musky', 'Nuts', 'Hanbang', 'Hinoki', 'Temple'
    ]

    with open(file_path, 'r', encoding='utf-8') as f:
        data = json.load(f)
    
    # DataFrame ë³€í™˜
    rows = []
    for item in data:
        row = {acc: 0.0 for acc in accords_33}
        row.update(item['mappings'])
        row['note'] = item['note']
        row['primary_accord'] = item['primary_accord']
        rows.append(row)
    
    df = pd.DataFrame(rows)
    df_accords = df[accords_33]

    # --- [ì§€í‘œ 4ë²ˆ] ì–´ì½”ë“œ ì»¤ë²„ë¦¬ì§€ ë¶„ì„ (Entropy Score) ---
    accord_sums = df_accords.sum()
    probabilities = accord_sums / accord_sums.sum()
    entropy = scipy.stats.entropy(probabilities, base=2)
    max_entropy = np.log2(len(accords_33))
    coverage_score = (entropy / max_entropy) * 100  # 100%ì— ê°€ê¹Œìš¸ìˆ˜ë¡ ê³ ë¥¸ ë¶„í¬

    # --- [ì§€í‘œ 2ë²ˆ] ë²¡í„° ê³µê°„ ì •í•©ì„± ë¶„ì„ (Similarity Heatmap) ---
    similarity_matrix = cosine_similarity(df_accords.T)
    sim_df = pd.DataFrame(similarity_matrix, index=accords_33, columns=accords_33)

    # --- ì‹œê°í™” 1: í‰í–‰ ì¢Œí‘œ ê·¸ë˜í”„ (Parallel Coordinates) ---
    plt.figure(figsize=(16, 8))
    # ê°€ë…ì„±ì„ ìœ„í•´ 100ê°œì˜ ë…¸íŠ¸ë¥¼ ìƒ˜í”Œë§í•˜ì—¬ ì‹œê°í™”
    sample_df = df.sample(min(100, len(df)))
    pd.plotting.parallel_coordinates(sample_df[accords_33 + ['primary_accord']], 
                                     'primary_accord', colormap='tab20', alpha=0.4)
    plt.xticks(rotation=90)
    plt.title(f"Parallel Coordinates: Accord Coverage Analysis\n(Coverage Score: {coverage_score:.2f}%)", fontsize=15)
    plt.legend(bbox_to_anchor=(1.02, 1), loc='upper left', ncol=2, fontsize='small')
    plt.grid(axis='y', linestyle='--', alpha=0.3)
    plt.tight_layout()
    plt.savefig('performance_coverage_parallel.png')

    # --- ì‹œê°í™” 2: ì–´ì½”ë“œ ìœ ì‚¬ë„ íˆíŠ¸ë§µ (Heatmap) ---
    plt.figure(figsize=(14, 12))
    sns.heatmap(sim_df, cmap='coolwarm', center=0.5, annot=False)
    plt.title("Accord Similarity Heatmap: Semantic Consistency Analysis", fontsize=15)
    plt.tight_layout()
    plt.savefig('performance_consistency_heatmap.png')

    # ê²°ê³¼ ì¶œë ¥
    print(f"ğŸ“Š [ì§€í‘œ 4] ì–´ì½”ë“œ ì»¤ë²„ë¦¬ì§€ ì ìˆ˜: {coverage_score:.2f}% (ì—”íŠ¸ë¡œí”¼: {entropy:.2f})")
    print(f"ğŸ“Š [ì§€í‘œ 2] ê°€ì¥ ë…ë¦½ì ì¸ ì–´ì½”ë“œ: {accord_sums.idxmin()} / ê°€ì¥ ì§€ë°°ì ì¸ ì–´ì½”ë“œ: {accord_sums.idxmax()}")
    print(f"\nâœ… ì‹œê°í™” íŒŒì¼ ì €ì¥ ì™„ë£Œ: \n1. performance_coverage_parallel.png \n2. performance_consistency_heatmap.png")

if __name__ == "__main__":
    run_performance_analysis()