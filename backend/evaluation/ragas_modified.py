import os
import json
import asyncio
import pandas as pd
from dotenv import load_dotenv
from tqdm.asyncio import tqdm
from pydantic import BaseModel, Field # Pydantic ì¶”ê°€

# ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate

load_dotenv()

# 1. ì¶œë ¥ êµ¬ì¡° ì •ì˜ (Pydantic ëª¨ë¸)
class EvaluationResult(BaseModel):
    fact_score: float = Field(description="íŒ©íŠ¸ ì •í™•ë„ ì ìˆ˜ (0~1)")
    emotional_score: float = Field(description="ê°ì„±ì  ê·¼ê±° ì ìˆ˜ (0~1)")
    reasoning: str = Field(description="í‰ê°€ ì´ìœ ")

# 2. ì»¤ìŠ¤í…€ íŒì‚¬ í”„ë¡¬í”„íŠ¸ ì„¤ì •
JUDGE_PROMPT = """
ë‹¹ì‹ ì€ ë§¤ìš° ì—„ê²©í•˜ê³  ê°ê´€ì ì¸ í–¥ìˆ˜ ì„œë¹„ìŠ¤ í’ˆì§ˆ ê²€ìˆ˜ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. 
ì œê³µëœ [ë°ì´í„°ë² ì´ìŠ¤ ì •ë³´]ì™€ [ëª¨ë¸ì˜ ì¶”ì²œ ë‹µë³€]ì„ ë¹„êµí•˜ì—¬ ë‹¤ìŒ ë‘ ê°€ì§€ ì§€í‘œë¥¼ 0.1ì  ë‹¨ìœ„ë¡œ ì •ë°€ í‰ê°€í•˜ì„¸ìš”.

### 1. Fact Accuracy (0.0 ~ 1.0)
DBì— ê¸°ë¡ëœ ì‚¬ì‹¤ ì •ë³´ë¥¼ ì™œê³¡ ì—†ì´ ì „ë‹¬í–ˆëŠ”ì§€ í‰ê°€í•©ë‹ˆë‹¤.
- 1.0 (Perfect): ì´ë¦„, ë¸Œëœë“œ, ëª¨ë“  ì„±ë¶„ ì •ë³´ê°€ DBì™€ ì™„ë²½íˆ ì¼ì¹˜í•¨.
- 0.9 (Excellent): ì •ë³´ëŠ” ì •í™•í•˜ë‚˜, DBì— ìˆëŠ” í•µì‹¬ ì„±ë¶„ ì¤‘ 1~2ê°œë¥¼ ëˆ„ë½í•¨.
- 0.7~0.8 (Good): ì •ë³´ëŠ” ì¼ì¹˜í•˜ë‚˜, DBì— ì—†ëŠ” ì„±ë¶„ì„ 'ì¶”ì¸¡ì„±'ìœ¼ë¡œ ì–¸ê¸‰í•¨. (ì˜ˆ: "ë¨¸ìŠ¤í¬ê°€ ë“¤ì–´ìˆì„ ê²ƒ ê°™ì€ í¬ê·¼í•¨" - DBì— ë¨¸ìŠ¤í¬ ì—†ìŒ)
- 0.5 (Fair): ë¸Œëœë“œë‚˜ ì´ë¦„ì€ ë§ì§€ë§Œ, ì „í˜€ ë‹¤ë¥¸ í–¥ì¡°ë¡œ ì„¤ëª…í•¨.
- 0.0~0.4 (Poor): ì¡´ì¬í•˜ì§€ ì•ŠëŠ” í–¥ìˆ˜ì´ê±°ë‚˜ ë¸Œëœë“œëª…ì„ í‹€ë¦¼.

### 2. Emotional Grounding (0.0 ~ 1.0)
ê°ì„± ìˆ˜ì‹ì–´ê°€ ì‹¤ì œ ì„±ë¶„(Note)ì—ì„œ ë…¼ë¦¬ì ìœ¼ë¡œ ë„ì¶œë˜ì—ˆëŠ”ì§€ í‰ê°€í•©ë‹ˆë‹¤. (ë§‰ì—°í•œ ì¹­ì°¬ì€ ê°ì  ëŒ€ìƒì…ë‹ˆë‹¤.)
- 1.0 (Specific): íŠ¹ì • ì„±ë¶„ê³¼ ìˆ˜ì‹ì–´ê°€ 1:1ë¡œ ë§¤ìš° ì •êµí•˜ê²Œ ì—°ê²°ë¨. (ì˜ˆ: 'ì•Œë°í•˜ì´ë“œ' -> 'ì½”ëì„ ì°Œë¥´ëŠ” ì„œëŠ˜í•œ ê³µê¸°')
- 0.8~0.9 (Plausible): ì„±ë¶„ì—ì„œ ìœ ì¶” ê°€ëŠ¥í•˜ì§€ë§Œ ë‹¤ì†Œ ì¼ë°˜ì ì¸ í‘œí˜„ì„. (ì˜ˆ: 'ì‹œíŠ¸ëŸ¬ìŠ¤' -> 'ìƒí¼í•˜ê³  ê°€ë²¼ìš´')
- 0.6~0.7 (Vague): ë„ˆë¬´ í¬ê´„ì ì´ë¼ ì–´ë–¤ í–¥ìˆ˜ì—ë‚˜ ë¶™ì¼ ìˆ˜ ìˆëŠ” í‘œí˜„ì„. (ì˜ˆ: 'ê³ ê¸‰ìŠ¤ëŸ½ê³  ë§¤ë ¥ì ì¸')
- 0.4~0.5 (Weak): ì„±ë¶„ê³¼ ìˆ˜ì‹ì–´ ì‚¬ì´ì˜ ì—°ê²° ê³ ë¦¬ê°€ ì•½í•¨. (ì˜ˆ: 'ë¡œì¦ˆ' í–¥ì¸ë° 'ì°¨ê°€ìš´ ë„ì‹œ' ëŠë‚Œì´ë¼ê³  í‘œí˜„)
- 0.0~0.3 (Illogical): ì„±ë¶„ê³¼ ì •ë°˜ëŒ€ë˜ëŠ” ëŠë‚Œì„ ë¶€ì—¬í•¨. (ì˜ˆ: 'ìš°ë””/ë ˆë”' ì„±ë¶„ì¸ë° 'íˆ¬ëª…í•˜ê³  ë¬¼ê¸° ì–´ë¦°'ì´ë¼ê³  í‘œí˜„)

### ì±„ì  ì›ì¹™:
- ì¡°ê¸ˆì´ë¼ë„ ê·¼ê±°ê°€ ë¶€ì¡±í•˜ë©´ 1.0ì  ëŒ€ì‹  0.8~0.9ì ì„ ë¶€ì—¬í•˜ì—¬ ë³€ë³„ë ¥ì„ í™•ë³´í•˜ì„¸ìš”.
- 'Reasoning'ì—ëŠ” ê°ì ì´ ëœ êµ¬ì²´ì ì¸ ë‹¨ì–´ë‚˜ ë¬¸ì¥ì„ ëª…ì‹œí•˜ì„¸ìš”.

### ì¶œë ¥ í˜•ì‹ (ë°˜ë“œì‹œ ì•„ë˜ JSON í˜•ì‹ë§Œ ì¶œë ¥í•˜ì„¸ìš”):
{{
    "fact_score": 0.0,
    "emotional_score": 0.0,
    "reasoning": "0.1ì  ë‹¨ìœ„ì˜ êµ¬ì²´ì  ê°ì  ì‚¬ìœ  í¬í•¨"
}}

[ë°ì´í„°ë² ì´ìŠ¤ ì •ë³´]:
{context}

[ëª¨ë¸ì˜ ì¶”ì²œ ë‹µë³€]:
{answer}
"""

async def run_custom_evaluation(csv_path):
    if not os.path.exists(csv_path):
        print(f"âŒ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {csv_path}")
        return

    df = pd.read_csv(csv_path)
    
    # .with_structured_output ì— ì •ì˜í•œ Pydantic ëª¨ë¸ì„ ì „ë‹¬í•©ë‹ˆë‹¤.
    judge_llm = ChatOpenAI(model="gpt-4o-mini", temperature=0).with_structured_output(EvaluationResult)
    prompt = ChatPromptTemplate.from_template(JUDGE_PROMPT)
    
    results = []

    print(f"ğŸ§ ì»¤ìŠ¤í…€ ê°ì„± ì§€í‘œ í‰ê°€ ì‹œì‘ (ì´ {len(df)}ê±´)...")

    for _, row in tqdm(df.iterrows(), total=len(df), desc="Judging"):
        try:
            chain = prompt | judge_llm
            # ì´ì œ ê²°ê³¼ëŠ” ë”•ì…”ë„ˆë¦¬ê°€ ì•„ë‹Œ EvaluationResult ê°ì²´ë¡œ ë°˜í™˜ë©ë‹ˆë‹¤.
            eval_res = await chain.ainvoke({
                "context": row['retrieved_contexts'],
                "answer": row['response']
            })
            # ê°ì²´ë¥¼ ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜í•˜ì—¬ ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€
            results.append(eval_res.dict())
        except Exception as e:
            print(f"âš ï¸ ê°œë³„ í‰ê°€ ì¤‘ ì—ëŸ¬ ë°œìƒ: {e}")
            results.append({"fact_score": 0, "emotional_score": 0, "reasoning": f"Error: {e}"})

    # ê²°ê³¼ í•©ì¹˜ê¸°
    eval_df = pd.DataFrame(results)
    final_df = pd.concat([df, eval_df], axis=1)
    
    output_file = "backend/evaluation/custom_eval_result_final.csv"
    final_df.to_csv(output_file, index=False, encoding="utf-8-sig")
    print(f"âœ… í‰ê°€ ì™„ë£Œ! ê²°ê³¼ê°€ {output_file}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")

if __name__ == "__main__":
    asyncio.run(run_custom_evaluation("backend/evaluation/evaluation_result.csv"))