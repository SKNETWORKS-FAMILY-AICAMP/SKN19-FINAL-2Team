import os
import json
import asyncio
import pandas as pd
from datasets import Dataset
from ragas import evaluate
from ragas.metrics import (
    faithfulness,
    answer_relevancy,
    context_precision,
    context_recall,
)
from dotenv import load_dotenv
from tqdm.asyncio import tqdm

# --- [ìˆ˜ì •ëœ ë¶€ë¶„ 1: ë˜í¼ ì„í¬íŠ¸] ---
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from ragas.llms import LangchainLLMWrapper
from ragas.embeddings import LangchainEmbeddingsWrapper
# ------------------------------------

import sys
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
from graph import build_graph

load_dotenv()

async def run_evaluation():
    test_file = "backend/evaluation/test_set.json"
    with open(test_file, "r", encoding="utf-8") as f:
        test_data = json.load(f)

    # --- [ìˆ˜ì •ëœ ë¶€ë¶„ 2: ê°ì²´ë¥¼ Ragas ë˜í¼ë¡œ ê°ì‹¸ê¸°] ---
    # 1. ë¨¼ì € LangChain ê°ì²´ ìƒì„±
    llm_obj = ChatOpenAI(model="gpt-4o-mini", temperature=0)
    emb_obj = OpenAIEmbeddings(model="text-embedding-3-small")

    # 2. Ragas ì „ìš© ë˜í¼ë¡œ ê°ì‹¸ê¸° (ì—ëŸ¬ ë°©ì§€ í•µì‹¬)
    eval_llm = LangchainLLMWrapper(llm_obj)
    eval_embeddings = LangchainEmbeddingsWrapper(emb_obj)
    # -----------------------------------------------
    
    workflow = build_graph()
    
    questions, answers, contexts, ground_truths = [], [], [], []

    print(f"ğŸš€ ì´ {len(test_data)}ê°œì˜ í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ ì‹¤í–‰ ì‹œì‘...")

    for i, data in enumerate(tqdm(test_data, desc="Running Graph")):
        question = data["question"]
        ground_truth = data["ground_truth"]
        
        config = {"configurable": {"thread_id": f"eval_test_{i}"}}
        inputs = {"user_query": question}
        
        try:
            final_state = await workflow.ainvoke(inputs, config=config)
            answer = final_state.get("final_response", "")
            raw_research = final_state.get("research_result", "")
            
            individual_contexts = [c.strip() for c in raw_research.split("-------------------------") if c.strip()]
            
            questions.append(question)
            answers.append(answer if answer else "No answer generated")
            contexts.append(individual_contexts if individual_contexts else ["No context found"])
            ground_truths.append(ground_truth)
            
        except Exception as e:
            print(f"âŒ ê·¸ë˜í”„ ì‹¤í–‰ ì—ëŸ¬: {e}")
            continue

    data_dict = {
        "question": questions,
        "answer": answers,
        "contexts": contexts,
        "ground_truth": ground_truths
    }
    dataset = Dataset.from_dict(data_dict)

    print("\nğŸ“Š Ragas ì§€í‘œ ê³„ì‚° ì¤‘...")
    try:
        # ìˆ˜ì •ëœ eval_llmê³¼ eval_embeddingsë¥¼ ì „ë‹¬
        result = evaluate(
            dataset,
            metrics=[
                faithfulness,
                answer_relevancy,
                context_precision,
                context_recall,
            ],
            llm=eval_llm,
            embeddings=eval_embeddings
        )

        df = result.to_pandas()
        print("\n[í‰ê°€ ê²°ê³¼ ìš”ì•½]")
        print(result)
        
        output_path = "backend/evaluation/evaluation_result.csv"
        df.to_csv(output_path, index=False, encoding="utf-8-sig")
        print(f"âœ… ê²°ê³¼ ì €ì¥ ì™„ë£Œ: {output_path}")

    except Exception as e:
        print(f"âŒ Ragas í‰ê°€ ì—ëŸ¬: {e}")

if __name__ == "__main__":
    asyncio.run(run_evaluation())