import json
import traceback
import re
from langgraph.graph import StateGraph, START, END
from langgraph.checkpoint.memory import MemorySaver 
from schemas import State
from database import METADATA
from tools import (
    client, 
    safe_json_parse, 
    execute_precise_search, 
    search_notes_vector, 
    search_exact_entity
)

# ==========================================
# âš™ï¸ ëª¨ë¸ ì„¤ì •
# ==========================================
FAST_MODEL = "gpt-4o-mini"
HIGH_PERFORMANCE_MODEL = "gpt-4o" 

# ==========================================
# 1. Supervisor (ë¼ìš°í„°)
# ==========================================
def supervisor(state: State) -> State:
    try:
        query = state["user_query"]
        
        print(f"\nğŸ“¡ [Supervisor] ì…ë ¥: '{query}'", flush=True)
        
        prompt = f"""
        ë‹¹ì‹ ì€ ëŒ€í™” íë¦„ì„ ì œì–´í•˜ëŠ” ê´€ë¦¬ìì…ë‹ˆë‹¤.
        
        [ì…ë ¥]
        - ì‚¬ìš©ì ë°œí™”: "{query}"
        
        [íŒë‹¨ ê¸°ì¤€]
        1. **interviewer**: í–¥ìˆ˜ ì¶”ì²œì„ ìœ„í•´ ì¶”ê°€ ì •ë³´ê°€ í•„ìš”í•œ ê²½ìš° (ì§ˆë¬¸).
        2. **researcher**: êµ¬ì²´ì  ì¶”ì²œ ìš”ì²­ì´ê±°ë‚˜ ì •ë³´ê°€ ì¶©ë¶„í•œ ê²½ìš°.
        3. **writer**: ë‹¨ìˆœ ì¸ì‚¬, ì¡ë‹´, ë˜ëŠ” ì¶”ì²œì´ ëë‚œ í›„ì˜ ë§ˆë¬´ë¦¬.
        
        ì‘ë‹µ(JSON): {{"route": "interviewer" | "researcher" | "writer"}}
        """
        
        msg = client.chat.completions.create(
            model=FAST_MODEL,
            messages=[{"role": "user", "content": prompt}],
            response_format={"type": "json_object"}
        )
        route = safe_json_parse(msg.choices[0].message.content).get("route", "writer")
        return {"route": route}
        
    except Exception:
        print("\nğŸš¨ [Supervisor Error]", flush=True)
        traceback.print_exc()
        return {"route": "writer"}

# ==========================================
# 2. Interviewer (ì •ë³´ ìˆ˜ì§‘)
# ==========================================
def interviewer(state: State) -> State:
    try:
        query = state["user_query"]
        current_context = state.get("interview_context", "") or ""
        
        print(f"\nğŸ¤ [Interviewer] ë‹µë³€ ë¶„ì„ ë° ë¬¸ë§¥ ì—…ë°ì´íŠ¸", flush=True)
        
        extraction_prompt = f"""
        ì‚¬ìš©ì ë‹µë³€ì—ì„œ í–¥ìˆ˜ ì¶”ì²œ ì •ë³´(ê³„ì ˆ, ì„±ë³„, ì·¨í–¥, ì´ë¯¸ì§€ ë“±)ë¥¼ ì¶”ì¶œí•´ ìš”ì•½í•˜ì„¸ìš”.
        - ê¸°ì¡´ ì •ë³´: {current_context}
        - ì‚¬ìš©ì ë‹µë³€: {query}
        í˜•ì‹ ì˜ˆ: "ì„±ë³„: ë‚¨ì„±, ì´ë¯¸ì§€: ì°¨ë„ë‚¨, ê³„ì ˆ: ê²¨ìš¸"
        """
        msg = client.chat.completions.create(
            model=FAST_MODEL,
            messages=[{"role": "user", "content": extraction_prompt}]
        )
        updated_context = msg.choices[0].message.content
        print(f"   ğŸ‘‰ ì—…ë°ì´íŠ¸ëœ ì •ë³´: {updated_context}", flush=True)
        
        judge_prompt = f"""
        ì¶”ì²œ ê²€ìƒ‰ì´ ê°€ëŠ¥í•œê°€ìš”? (ìµœì†Œí•œ í–¥ ì·¨í–¥, ë¸Œëœë“œ, ë¶„ìœ„ê¸° ì¤‘ í•˜ë‚˜ ì¡´ì¬)
        ì •ë³´: {updated_context}
        ì‘ë‹µ(JSON): {{"is_sufficient": true/false, "next_question": "ì§ˆë¬¸ ë‚´ìš©"}}
        """
        judge_msg = client.chat.completions.create(
            model=FAST_MODEL,
            messages=[{"role": "user", "content": judge_prompt}],
            response_format={"type": "json_object"}
        )
        judge_result = safe_json_parse(judge_msg.choices[0].message.content)
        
        if judge_result.get("is_sufficient"):
            print("   âœ… ì •ë³´ ì¶©ë¶„ -> Researcherë¡œ ì „ë‹¬", flush=True)
            return {
                "route": "researcher", 
                "interview_context": updated_context,
                "user_query": f"{updated_context} (ì‚¬ìš©ì ì˜ë„ ë°˜ì˜ ì¶”ì²œ)" 
            }
        else:
            print("   â“ ì •ë³´ ë¶€ì¡± -> ì‚¬ìš©ìì—ê²Œ ì¬ì§ˆë¬¸", flush=True)
            return {
                "route": "end",
                "interview_context": updated_context,
                "final_response": judge_result.get("next_question")
            }
            
    except Exception:
        print("\nğŸš¨ [Interviewer Error]", flush=True)
        traceback.print_exc()
        return {"route": "writer", "final_response": "ì ì‹œ ë¬¸ì œê°€ ìƒê²¼ìŠµë‹ˆë‹¤. ë‹¤ì‹œ ë§ì”€í•´ ì£¼ì‹œê² ì–´ìš”?"}

# ==========================================
# 3. Researcher (ì „ëµ ìˆ˜ë¦½)
# ==========================================
def researcher(state: State) -> State:
    try:
        query = state["user_query"]
        print(f"\nğŸ•µï¸ [Researcher] ìƒí™©ë³„ ë§ì¶¤ ì „ëµ ìˆ˜ë¦½: {query}", flush=True)

        meta_summary = {k: v[:20] for k, v in METADATA.items()}

        prompt = f"""
        ë‹¹ì‹ ì€ ìµœê³ ì˜ í¼ìŠ¤ë„ í¼í“¸ ì»¨ì„¤í„´íŠ¸ì…ë‹ˆë‹¤.
        ì‚¬ìš©ì ìš”ì²­: "{query}"
        DB ë©”íƒ€ë°ì´í„°: {json.dumps(meta_summary, ensure_ascii=False)}
        
        [ì„ë¬´]
        ì‚¬ìš©ìì˜ ìš”ì²­ ì˜ë„ë¥¼ ë¶„ì„í•˜ê³ , ì•„ë˜ **[ì‹œë‚˜ë¦¬ì˜¤ ë¼ì´ë¸ŒëŸ¬ë¦¬]** ì¤‘ ê°€ì¥ ì í•©í•œ í•˜ë‚˜ë¥¼ ê³¨ë¼ 3ê°€ì§€ ê²€ìƒ‰ ì „ëµ(Plan)ì„ ìˆ˜ë¦½í•˜ì„¸ìš”.
        
        === [ì‹œë‚˜ë¦¬ì˜¤ ë¼ì´ë¸ŒëŸ¬ë¦¬] ===
        Type A. [ì´ë¯¸ì§€/ë¶„ìœ„ê¸°] (ì˜ˆ: "ì°¨ê°€ìš´ ë„ì‹œ ë‚¨ì", "ì²­ìˆœí•œ ëŠë‚Œ")
        Type B. [íŠ¹ì • í–¥ë£Œ/ë…¸íŠ¸] (ì˜ˆ: "ì¥ë¯¸ í–¥ ì¢‹ì•„í•´", "ìš°ë””í•œ ê±°")
        Type C. [TPO/ìƒí™©] (ì˜ˆ: "ì†Œê°œíŒ…", "ë°ì¼ë¦¬", "ë©´ì ‘")
        Type D. [ìœ ì‚¬ í–¥ìˆ˜ ì°¾ê¸°] (ì˜ˆ: "ìƒ¤ë„¬ ë„˜ë²„5 ê°™ì€ ê±°")
        Type E. [ì„ ë¬¼/ì…ë¬¸/ì •ë³´ë¶€ì¡±] (ì˜ˆ: "ì—¬ì¹œ ì„ ë¬¼", "ì…ë¬¸ìš©")
        
        === [ì‘ì„± ê·œì¹™] ===
        1. 3ê°œì˜ Planì„ ì‘ì„±í•˜ì„¸ìš”.
        2. **strategy_name**ì€ ì „ëµ ì´ë¦„(ì˜ˆ: "ì§ê´€ì  ì¼ì¹˜")ì„ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•˜ì„¸ìš”.
        3. **í•„ìˆ˜**: ë…¸íŠ¸(Note) í‚¤ì›Œë“œëŠ” ë°˜ë“œì‹œ **ì˜ì–´(English)**ë¡œ ë³€í™˜í•˜ì„¸ìš”.
        4. ì¶”ìƒì  í‘œí˜„ì€ 'use_vector_search': trueë¡œ ì„¤ì •í•˜ì„¸ìš”.
        
        ì‘ë‹µ(JSON) ì˜ˆì‹œ:
        {{
            "scenario_type": "Type A",
            "plans": [
                {{
                    "priority": 1,
                    "strategy_name": "ì§ê´€ì  ì¼ì¹˜",
                    "filters": [],
                    "note_keywords": ["Mint", "Aquatic"],
                    "use_vector_search": true
                }},
                ... (ì´ 3ê°œ)
            ]
        }}
        """
        
        msg = client.chat.completions.create(
            model=HIGH_PERFORMANCE_MODEL,
            messages=[{"role": "user", "content": prompt}],
            response_format={"type": "json_object"}
        )
        
        parsed = safe_json_parse(msg.choices[0].message.content)
        plans = parsed.get("plans", []) if parsed else []
        scenario_type = parsed.get("scenario_type", "Unknown")
        
        print(f"   ğŸ’¡ ì„ íƒëœ ì‹œë‚˜ë¦¬ì˜¤: {scenario_type}", flush=True)
        
        search_logs = []
        final_result_text = ""
        
        for plan in plans:
            priority = plan.get('priority', '?')
            strategy = plan.get('strategy_name', f"Strategy-{priority}")
            
            print(f"   ğŸ‘‰ [Priority {priority}] ì‹¤í–‰: {strategy}", flush=True)
            
            current_filters = []
            
            # ë°©ì–´ ì½”ë“œ: í•„í„°ê°€ ë¬¸ìì—´ ë“±ìœ¼ë¡œ ì˜ëª» ì˜¤ë©´ ë¬´ì‹œ
            for f in plan.get("filters", []):
                if not isinstance(f, dict):
                    print(f"   âš ï¸ [Warning] ì˜ëª»ëœ í•„í„° í˜•ì‹ ë¬´ì‹œë¨: {f}", flush=True)
                    continue
                    
                col = f.get('column')
                val = f.get('value')
                if not col or not val: continue
                
                if col in ['brand', 'perfume_name']:
                    corrected = search_exact_entity(val, col)
                    if corrected: f['value'] = corrected
                current_filters.append(f)
            
            notes = []
            if plan.get("use_vector_search"):
                notes.extend(search_notes_vector(query, top_k=3))
            
            for k in plan.get("note_keywords", []):
                notes.extend(search_notes_vector(k, top_k=2))
                
            if notes:
                current_filters.append({"column": "note", "value": list(set(notes))})
            
            # ê²€ìƒ‰ ì‹¤í–‰
            result_text = execute_precise_search(current_filters)
            
            if result_text:
                print(f"     âœ… ê²°ê³¼ í™•ë³´", flush=True)
                search_logs.append(f"ì „ëµ [{strategy}] ì„±ê³µ")
                final_result_text += f"\n=== [ì „ëµ: {strategy}] ===\n{result_text}\n"
            else:
                print(f"     âŒ ê²°ê³¼ ì—†ìŒ", flush=True)
                search_logs.append(f"ì „ëµ [{strategy}] ê²°ê³¼ ì—†ìŒ")
        
        if not final_result_text:
            final_result_text = "ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤."

        return {
            "search_plans": plans,
            "search_logs": search_logs,
            "research_result": final_result_text,
            "route": "writer"
        }
        
    except Exception:
        print("\nğŸš¨ [Researcher Error]", flush=True)
        traceback.print_exc()
        return {"research_result": "ì˜¤ë¥˜ ë°œìƒ", "route": "writer"}


# ==========================================
# 4. Writer (ê¸€ì“°ê¸°)
# ==========================================
def writer(state: State) -> State:
    try:
        print("âœï¸ [Writer] ë‹µë³€ ì‘ì„±", flush=True)
        query = state["user_query"]
        result = state.get("research_result", "")
        
        prompt = f"""
        ë‹¹ì‹ ì€ í–¥ìˆ˜ë¥¼ ì˜ ëª¨ë¥´ëŠ” ì´ˆë³´ìë¥¼ ìœ„í•œ ì„¸ìƒì—ì„œ ê°€ì¥ ì¹œì ˆí•œ í–¥ìˆ˜ ì»¨ì„¤í„´íŠ¸ì…ë‹ˆë‹¤.
        
        [ì‚¬ìš©ì ìš”ì²­]: "{query}"
        
        [ê²€ìƒ‰ëœ í–¥ìˆ˜ ë°ì´í„°]: 
        {result}
        
        [ì‘ì„± ê·œì¹™ - í•„ë…]
        1. **ëª©ì°¨ ìŠ¤íƒ€ì¼**: 
           - 'ì „ëµ:' ë‹¨ì–´ ê¸ˆì§€.
           - **`## ë²ˆí˜¸. [ì „ëµì´ë¦„] ë¸Œëœë“œ - í–¥ìˆ˜ëª…`** í˜•ì‹ ì—„ìˆ˜.
           - ì˜ˆ: `## 1. [ì„¸ë ¨ëœ ë‹ˆì¹˜] Loewe - Agua de Loewe Ella`
        
        2. **ì´ë¯¸ì§€ í•„ìˆ˜**: `![í–¥ìˆ˜ëª…](ì´ë¯¸ì§€ë§í¬)`
        
        3. **[â˜…ë§¤ìš° ì¤‘ìš”â˜…] ì„œì‹ ë° ê°•ì¡° ê·œì¹™**:
           - **í•­ëª© ì œëª©(Label)**: ë°˜ë“œì‹œ **`_` (ì–¸ë”ë°”)**ë¡œ ê°ì‹¸ì„¸ìš”. (íŒŒë€ìƒ‰ ì œëª©)
             - ì˜ˆ: `_ì–´ë–¤ í–¥ì¸ê°€ìš”?_`, `_ì¶”ì²œ ì´ìœ _`, `_ì •ë³´_`
           - **ë‚´ìš© ê°•ì¡°(Highlight)**: í•µì‹¬ ë‹¨ì–´ëŠ” **`**` (ë³„í‘œ 2ê°œ)**ë¡œ ê°ì‹¸ì„¸ìš”. (í•‘í¬ìƒ‰ ê°•ì¡°)
             - ì˜ˆ: `ì²˜ìŒì—” **ìƒí¼í•œ ê·¤ í–¥**ì´ ë‚˜ìš”.`
        
        4. **êµ¬ë¶„ì„ **: í–¥ìˆ˜ ì¶”ì²œ ì‚¬ì´ì— `---` ì‚½ì….
        
        5. **ì •ë³´ í‘œê¸°**: ë¸Œëœë“œ, ì´ë¦„, ì¶œì‹œë…„ë„ë§Œ ê¸°ì¬.
        
        6. **[í•„ìˆ˜] í–¥ ì„¤ëª… ë°©ì‹ (ì‹œê°„ ìˆœì„œ)**:
           - **"ì²˜ìŒì—ëŠ” ~(íƒ‘), ì‹œê°„ì´ ì§€ë‚˜ë©´ ~(ë¯¸ë“¤), ëìœ¼ë¡œ ~(ë² ì´ìŠ¤)"** ìˆœì„œë¡œ ì„¤ëª…í•˜ì„¸ìš”.
           - ì „ë¬¸ ìš©ì–´(ë…¸íŠ¸, ì–´ì½”ë“œ ë“±) ëŒ€ì‹  ì‰¬ìš´ ë¹„ìœ ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”.
           
        7. **[í•µì‹¬] ì¶”ì²œ ë…¼ë¦¬ ì—°ê²° (Why?)**:
           - `_ì¶”ì²œ ì´ìœ _`ë¥¼ ì‘ì„±í•  ë•Œ, ë‹¨ìˆœíˆ "ì¢‹ì•„ìš”"ë¼ê³  í•˜ì§€ ë§ˆì„¸ìš”.
           - **[ì‚¬ìš©ì ì§ˆë¬¸ì˜ í‚¤ì›Œë“œ(ë‚˜ì´, ì„±ë³„, ìƒí™©)]**ì™€ **[í–¥ìˆ˜ì˜ íŠ¹ì§•]**ì„ ë…¼ë¦¬ì ìœ¼ë¡œ ì—°ê²°í•´ì„œ ì„¤ëª…í•˜ì„¸ìš”.
           - ì˜ˆì‹œ 1: "20ëŒ€ ì—¬ì„±ë¶„ì—ê²Œ ì„ ë¬¼í•˜ì‹ ë‹¤ê³  í•˜ì…¨ì£ ? ì´ ë‚˜ì´ëŒ€ì—ëŠ” ë„ˆë¬´ ë¬´ê±°ìš´ í–¥ë³´ë‹¤ëŠ” **ìƒí¼í•œ ê³¼ì¼ í–¥**ì´ ìƒê¸° ë°œë„í•œ ì´ë¯¸ì§€ë¥¼ ì¤˜ì„œ í˜¸ë¶ˆí˜¸ ì—†ì´ ì˜ ì–´ìš¸ë ¤ìš”."
           - ì˜ˆì‹œ 2: "ì†Œê°œíŒ…ìš© í–¥ìˆ˜ë¥¼ ì°¾ìœ¼ì…¨ëŠ”ë°, ì´ í–¥ì˜ **ì€ì€í•œ ë¹„ëˆ„ ì”í–¥**ì´ ìƒëŒ€ë°©ì—ê²Œ **ê¹”ë”í•˜ê³  ë‹¨ì •í•œ ì¸ìƒ**ì„ ì‹¬ì–´ì£¼ê¸°ì— ì™„ë²½í•´ìš”."
        
        [ì¶œë ¥ í˜•ì‹ ì˜ˆì‹œ]
        
        ì•ˆë…•í•˜ì„¸ìš”! ìš”ì²­í•˜ì‹  ëŠë‚Œì— ë§ì¶° 3ê°€ì§€ í–¥ìˆ˜ë¥¼ ê³¨ë¼ë´¤ì–´ìš”.
        
        ## 1. [ê¹¨ë—í•œ ë¹„ëˆ„] **Santa Maria Novella - Fresia**
        ![Fresia](ë§í¬)
        
        - _ì–´ë–¤ í–¥ì¸ê°€ìš”?_: ì²˜ìŒì—” **ë§‰ ì”»ê³  ë‚˜ì˜¨ ë“¯í•œ ë¹„ëˆ„ ê±°í’ˆ ëƒ„ìƒˆ**ê°€ í™• í’ê²¨ìš”. ì‹œê°„ì´ ì§€ë‚˜ë©´ **ì€ì€í•œ ìƒí™” ê½ƒí–¥ê¸°**ê°€ ì˜¬ë¼ì˜¤ê³ , ë§ˆì§€ë§‰ì—” **í¬ê·¼í•œ ì‚´ëƒ„ìƒˆ**ê°€ ë‚¨ì•„ìš”.
        - _ì¶”ì²œ ì´ìœ _: **20ëŒ€ ì—¬ì„±ë¶„**ì—ê²Œ ì„ ë¬¼í•˜ê¸° ê°€ì¥ ì¢‹ì€ í–¥ì´ì—ìš”. **ê³¼í•˜ì§€ ì•Šì€ ê¹¨ë—í•¨**ì´ ì²­ìˆœí•œ ì´ë¯¸ì§€ë¥¼ ë§Œë“¤ì–´ì¤˜ì„œ ë°ì¼ë¦¬ë¡œ ì“°ê¸° ë”± ì¢‹ê±°ë“ ìš”.
        - _ì •ë³´_: Santa Maria Novella / Fresia / 1993ë…„ ì¶œì‹œ
        
        ---
        ...
        """
        
        msg = client.chat.completions.create(
            model=HIGH_PERFORMANCE_MODEL, 
            messages=[{"role": "user", "content": prompt}]
        )
        
        raw_content = msg.choices[0].message.content
        
        # [í›„ì²˜ë¦¬] ê°•ì¡° ê³µë°± ì œê±°
        fixed_content = re.sub(r'\*\*\s*(.*?)\s*\*\*', r'**\1**', raw_content)
        
        return {"final_response": fixed_content}
    except Exception:
        print("\nğŸš¨ [Writer Error]", flush=True)
        traceback.print_exc()
        return {"final_response": "ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."}


# ==========================================
# Graph Build
# ==========================================
def build_graph():
    graph = StateGraph(State)
    
    graph.add_node("supervisor", supervisor)
    graph.add_node("interviewer", interviewer)
    graph.add_node("researcher", researcher)
    graph.add_node("writer", writer)
    
    graph.add_edge(START, "supervisor")
    
    def route_decision(state: State):
        return state["route"]
    
    graph.add_conditional_edges(
        "supervisor",
        route_decision,
        {"interviewer": "interviewer", "researcher": "researcher", "writer": "writer"}
    )
    
    graph.add_conditional_edges(
        "interviewer",
        route_decision,
        {"researcher": "researcher", "end": END}
    )
    
    graph.add_edge("researcher", "writer")
    graph.add_edge("writer", END)
    
    # ë©”ëª¨ë¦¬ ì €ì¥ì†Œ(Checkpointer) ì ìš©
    memory = MemorySaver()
    
    return graph.compile(checkpointer=memory)